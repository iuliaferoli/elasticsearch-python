{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Large Language Models in Elasticsearch\n",
    "\n",
    "This notebook shows a few examples of importing LLMs into your Elastic environment and using the `inference` API to enrich your index with more semantic context (embeddings, sentiment score, etc).\n",
    "\n",
    "You can:\n",
    "* Use `eland` and `docker` to deploy compatible models from `Huggingface` into your cluster\n",
    "* Use the `inference` API to make calls to a model and get back the result\n",
    "* Use `pipelines` to generate embeddings or other semantic data for your entire index\n",
    "* Use the newly generated fields as part of your semantic or hybrid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting to the Elasticsearch cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iulia/python/cheat sheet/elasticsearch-python/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/var/folders/k3/sm4kh0y91fs5_fft4375wwh00000gn/T/ipykernel_36806/1504620431.py:2: DeprecationWarning: Importing from the 'elasticsearch.client' module is deprecated. Instead use 'elasticsearch' module for importing the client.\n",
      "  from elasticsearch.client import MlClient\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.client import MlClient\n",
    "from getpass import getpass\n",
    "\n",
    "#Connect to the elastic cloud server\n",
    "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
    "ELASTIC_API_KEY = getpass(\"Elastic API Key: \")\n",
    "\n",
    "# Create an Elasticsearch client using the provided credentials\n",
    "client = Elasticsearch(\n",
    "    cloud_id=ELASTIC_CLOUD_ID,  # cloud id can be found under deployment management\n",
    "    api_key=ELASTIC_API_KEY, # your username and password for connecting to elastic, found under Deplouments - Security\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For NLP use cases we can import [models from the Hugging Face model hub](https://huggingface.co/elastic/distilbert-base-uncased-finetuned-conll03-english)\n",
    "\n",
    "You can add them to the Elasticsearch cluster via docker.\n",
    "\n",
    "https://www.elastic.co/guide/en/elasticsearch/client/eland/current/machine-learning.html#ml-nlp-pytorch-docker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull docker.elastic.co/eland/eland\n",
    "\n",
    "!docker run -it --rm elastic/eland \\\n",
    "    eland_import_hub_model \\\n",
    "      --cloud-id $ELASTIC_CLOUD_ID \\\n",
    "      --es-api-key $ELASTIC_API_KEY \\\n",
    "      --hub-model-id distilbert-base-uncased-finetuned-sst-2-english \\\n",
    "      --task-type text_classification \\\n",
    "      --start "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See all models successfuly imported into your instance (which are also available through the UI)\n",
    "\n",
    "https://www.elastic.co/guide/en/elasticsearch/reference/current/get-trained-models.html?#ml-get-trained-models-request "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"devrel\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test the models by running a query via the clients or (sometimes quicker and easier) through the Dev Console on Elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_id = \"sentence-transformers__msmarco-minilm-l-12-v3\"\n",
    "model_id = \".elser_model_1\"\n",
    "models = MlClient.get_trained_models(client, model_id=model_id)\n",
    "models.body\n",
    "\n",
    "#Run a query againt the model - this is the format the query imput must be used in, you can later map your features into this format through an ingest pipeline\n",
    "doc_test = {\"text_field\": \"This is a very nice sentence\"}\n",
    "\n",
    "result = MlClient.infer_trained_model(client, model_id =model_id, docs = doc_test)\n",
    "result[\"inference_results\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines to add LLM insights to your index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to run your model against the entire index, you can build a pipeline to either run at the initial data ingestion, or to reindex your existing data with the new LLM insights.\n",
    "\n",
    "You can build a pipeline for each model, or put them all together if you want multiple types of fields added at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.ingest.put_pipeline(\n",
    "    id=\"sentiment_and_embeddings\", \n",
    "    processors=[\n",
    "    { #sentiment analysis example\n",
    "      \"inference\": {\n",
    "        \"model_id\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "        \"target_field\" : \"sentiment\",\n",
    "        \"field_map\": {\n",
    "          \"Sentence\": \"text_field\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    { #ELSER sparse vector example\n",
    "      \"inference\": {\n",
    "        \"model_id\": \".elser_model_1\",\n",
    "        \"target_field\": \"ml\",\n",
    "        \"field_map\": {\n",
    "          \"Sentence\": \"text_field\"\n",
    "        },\n",
    "        \"inference_config\": {\n",
    "          \"text_expansion\": {\n",
    "            \"results_field\": \"tokens\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    { #embedding to dense vector example\n",
    "      \"inference\": {\n",
    "        \"model_id\": \"sentence-transformers__msmarco-minilm-l-12-v3\",\n",
    "        \"target_field\" : \"text_embedding\",\n",
    "        \"field_map\": {\n",
    "          \"Sentence\": \"text_field\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the new enriched index, we need to add a field to the mappings where the inference results will be stored. \n",
    "Here are some examples for different types of models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {\n",
    "  \"properties\": {\n",
    "    #dense vectors for embeddings\n",
    "    \"text_embedding.predicted_value\": {\n",
    "      \"type\": \"dense_vector\",\n",
    "      \"dims\": 384,\n",
    "      \"index\": True,\n",
    "      \"similarity\": \"cosine\"\n",
    "    },\n",
    "    \"Character\": {\n",
    "        \"type\": \"text\"\n",
    "    },\n",
    "    \"Line_number\": {\n",
    "      \"type\": \"long\"\n",
    "    },\n",
    "    \"Sentence\": {\n",
    "      \"type\": \"text\"\n",
    "     },\n",
    "     #sentiment results field\n",
    "    \"sentiment\": {\n",
    "      \"properties\": {\n",
    "        \"model_id\": {\n",
    "          \"type\": \"text\",\n",
    "          \"fields\": {\n",
    "            \"keyword\": {\n",
    "              \"type\": \"keyword\",\n",
    "              \"ignore_above\": 256\n",
    "              }\n",
    "          }\n",
    "        },\n",
    "        \"predicted_value\": {\n",
    "          \"type\": \"text\",\n",
    "          \"fields\": {\n",
    "            \"keyword\": {\n",
    "              \"type\": \"keyword\",\n",
    "              \"ignore_above\": 256\n",
    "            }\n",
    "          }\n",
    "        },\n",
    "        \"prediction_probability\": {\n",
    "          \"type\": \"float\"\n",
    "        }\n",
    "      }\n",
    "    }, #ELSER sparse vectors\n",
    "    \"ml.tokens\": { \n",
    "      \"type\": \"rank_features\" \n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the new index with enriched data\n",
    "client.indices.create(index=index, mappings=mappings)\n",
    "client.reindex(body={\n",
    "      \"source\": {\n",
    "          \"index\": \"index_name\"},\n",
    "      \"dest\": {\"index\": \"index_name_new\", \"pipeline\" : \"sentiment\"}\n",
    "    }, wait_for_completion=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries based on LLM fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example searching using the new sentiment field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query={\n",
    "    \"match\" : {\n",
    "      \"sentiment.predicted_value\": \"NEGATIVE\"\n",
    "    }\n",
    "  }\n",
    "response = client.search(index = \"index_name_new\",query=query, sort=\"sentiment.prediction_probability:desc\")\n",
    "print(\"The most negative sentences in the series:\")\n",
    "for hit in response[\"hits\"][\"hits\"]:\n",
    "    print(hit['_source'][\"Sentence\"],  hit['_source'][\"sentiment\"][\"prediction_probability\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example searching using the new embeddings field\n",
    "\n",
    "In this case we also have to first convert the query text to the same embeddings space by also running this sentence against our inference model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what languages do you use\"\n",
    "\n",
    "question_embedded = MlClient.infer_trained_model(client, model_id =model_id, docs = question)\n",
    "query_vector = question_embedded[\"inference_results\"][0][\"predicted_value\"]\n",
    "\n",
    "query = {\n",
    "        \"field\": \"text_embedding.predicted_value\",\n",
    "        \"query_vector\": query_vector,\n",
    "        \"k\": 5,\n",
    "        \"num_candidates\": 100\n",
    "        }\n",
    "        \n",
    "result = client.search(index = index, knn=query, source=[\"Sentence\"])\n",
    "\n",
    "for element in result[\"hits\"][\"hits\"]:\n",
    "    print(\"{}: {}, score {}\".format(element[\"_source\"][\"Sentence\"], element[\"_score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using ELSER, you don't need the extra step of embedding the question yourself, as this is done automatically. \n",
    "\n",
    "So the model takes in the string question directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.search(\n",
    "    index=index, \n",
    "    size=5,\n",
    "    query={\n",
    "        \"text_expansion\": {\n",
    "            \"ml.tokens\": {\n",
    "                \"model_id\":\".elser_model_1\",\n",
    "                \"model_text\":question\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "for element in result[\"hits\"][\"hits\"]:\n",
    "    print(\"{}: {}, score {}\".format(element[\"_source\"][\"Sentence\"], element[\"_score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also combine multiple types of search - like the ELSER semantic search with the sentiment field and some \"classic\" filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.search(\n",
    "    index=index, \n",
    "    size=5,\n",
    "    query={\n",
    "        \"bool\": {\n",
    "            \"should\": [{\n",
    "                \"text_expansion\": {\n",
    "                    \"ml.tokens\": {\n",
    "                        \"model_id\":\".elser_model_1\",\n",
    "                        \"model_text\":\"brave\"\n",
    "                    }\n",
    "                },\n",
    "            }],\n",
    "            \"must\":[\n",
    "            {\n",
    "                \"match\" : {\n",
    "                    \"sentiment.predicted_value\": \"NEGATIVE\"\n",
    "                }\n",
    "            }],\n",
    "            \"must_not\":[\n",
    "                    {\"term\":{\n",
    "                        \"Sentence\":\"fear\"\n",
    "                 }}]\n",
    "        }\n",
    "    })\n",
    "\n",
    "for element in result[\"hits\"][\"hits\"]:\n",
    "        print(\"{}: {}\".format(element[\"_source\"][\"Character\"], element[\"_source\"][\"Sentence\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
